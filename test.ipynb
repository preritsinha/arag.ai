{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arag.ai<br>\n",
    "Active RAG based LLM Model<br>\n",
    "Built so that it's dependent on you. Hence works only for you.<br>\n",
    "Created with ❤️<br><br>\n",
    "Flow - \n",
    "1. Toggle for RAG and Generic LLM<br>if RAG: <br>\n",
    "2. get the link to docs for embedding \n",
    "3. query \n",
    "4. result generation <br>else: <br>\n",
    "2. query \n",
    "3. result generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv('TOKEN')\n",
    "print(token)\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.content\n",
    "data = query({\"inputs\":\"What is Task Decomposition\"})\n",
    "data_str = data.decode('utf-8')  # Decode bytes to string\n",
    "response_json = json.loads(data_str)  # Parse the JSON string\n",
    "\n",
    "print(\"\\n\".join(response_json[0]['generated_text'].split('\\n')[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "token = os.getenv('TOKEN')\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "# def query(payload):\n",
    "#     response = requests.post(API_URL, headers=headers, json=payload)\n",
    "#     return response.content\n",
    "# data = query({\"question\",\"Who is the best player in football?\"})\n",
    "# data_str = data.decode('utf-8')  # Decode bytes to string\n",
    "# response_json = json.loads(data_str)  # Parse the JSON string\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "\n",
    "prompt = \"What's the capital of India\"\n",
    "formatted_docs = \"New Delhi is captial of India\"\n",
    "output = query({\"inputs\": {\"question\": f\"{prompt}\",\"context\": f\"{formatted_docs})\"}})\n",
    "print(output['answer'])\n",
    "# print(response_json)#[0][\"generated_text\"].split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "token = os.getenv('TOKEN')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", token=token)\n",
    "generator = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "prompt = \"capital of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "\t\"question\": \"What is my name?\",\n",
    "\t\"context\": \"My name is Clara and I live in Berkeley.\"\n",
    "}\n",
    "generated_text = generator(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load a compatible model from sentence-transformers\n",
    "model = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "\n",
    "# Wrapper class for embeddings\n",
    "class EmbeddingsWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_tensor=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_tensor=True).tolist()[0]\n",
    "\n",
    "# Instantiate the wrapper\n",
    "embedding_wrapper = EmbeddingsWrapper(model)\n",
    "\n",
    "# Load Docs\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title', 'post-header')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and create vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_wrapper)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Retrieve documents and format them\n",
    "retrieved_docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "formatted_docs = format_docs(retrieved_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/valhalla/bart-large-finetuned-squadv1\"\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "\n",
    "prompt = \"Summarize\"\n",
    "output = query({\"inputs\": {\"question\": f\"{prompt}\",\"context\": f\"{formatted_docs})\"}})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
